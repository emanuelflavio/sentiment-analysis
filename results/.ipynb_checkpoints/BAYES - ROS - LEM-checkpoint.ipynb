{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe08143c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'class_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 54>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m X_treino_res, y_treino_res \u001b[38;5;241m=\u001b[39m oversampler\u001b[38;5;241m.\u001b[39mfit_resample(X_treino_vect, y_treino)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Treinamento do classificador Random Forest\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mMultinomialNB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Validação cruzada\u001b[39;00m\n\u001b[0;32m     57\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_val_score(classifier, X_treino_res, y_treino_res, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# cv=5 indica 5-fold cross-validation\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'class_weight'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "# Carregar os dados\n",
    "data = pd.read_csv('new_comentarios_ingles.csv')\n",
    "df_1 = data['Comentario']\n",
    "rotulos = data['sentimento']\n",
    "\n",
    "# Definir polarizações de sentimento\n",
    "def define_sentiment(rating):\n",
    "    if rating == 'positivo':\n",
    "        return 1\n",
    "    elif rating == 'neutro':\n",
    "        return 2\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "data['sentimento_polarizado'] = data['sentimento'].apply(define_sentiment)\n",
    "rotulos = data['sentimento_polarizado']\n",
    "\n",
    "# Tokenização e lematização\n",
    "comentarios_tokenizados = []\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [wnl.lemmatize(plv) for plv in tokens]\n",
    "\n",
    "for tok in df_1:\n",
    "    tokens = nltk.word_tokenize(str(tok))\n",
    "    comentarios_tokenizados.append(tokens)\n",
    "\n",
    "lemm_tks = [lemmatize_tokens(tokens) for tokens in comentarios_tokenizados]\n",
    "\n",
    "# Divisão em conjuntos de treinamento e teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(lemm_tks, rotulos, test_size=0.2, random_state=42, stratify=rotulos)\n",
    "\n",
    "# Vetorização dos dados de texto\n",
    "vectorizer = CountVectorizer()\n",
    "X_treino_vect = vectorizer.fit_transform([' '.join(comentario) for comentario in X_treino])\n",
    "X_teste_vect = vectorizer.transform([' '.join(comentario) for comentario in X_teste])\n",
    "\n",
    "# Aplicando Random Oversampling\n",
    "oversampler = RandomOverSampler()\n",
    "X_treino_res, y_treino_res = oversampler.fit_resample(X_treino_vect, y_treino)\n",
    "\n",
    "# Treinamento do classificador Random Forest\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Validação cruzada\n",
    "scores = cross_val_score(classifier, X_treino_res, y_treino_res, cv=5)  # cv=5 indica 5-fold cross-validation\n",
    "\n",
    "# Treinamento final do modelo\n",
    "classifier.fit(X_treino_res, y_treino_res)\n",
    "\n",
    "# Predição nos dados de teste\n",
    "predicoes = classifier.predict(X_teste_vect)\n",
    "\n",
    "# Calcular precisão, recall e F-score\n",
    "precision = precision_score(y_teste, predicoes, average='weighted')\n",
    "recall = recall_score(y_teste, predicoes, average='weighted')\n",
    "f_score = f1_score(y_teste, predicoes, average='weighted')\n",
    "acuracia = accuracy_score(y_teste, predicoes)\n",
    "relatorio_classificacao = classification_report(y_teste, predicoes, target_names=['Negativo', 'Positivo', 'Neutro'])\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Resultados da validação cruzada:\")\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Precisão média:\", scores.mean())\n",
    "print(\"Acuracia: {:.2f}\".format(acuracia))\n",
    "print(\"Precisão: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F-score: {:.2f}\".format(f_score))\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(relatorio_classificacao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e71910fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Undersampling:\n",
      "Resultados da validação cruzada:\n",
      "Scores: [0.53119093 0.54820416 0.49716446 0.5        0.51325758]\n",
      "Precisão média: 0.517963424414275\n",
      "Acuracia: 0.56\n",
      "Precisão: 0.84\n",
      "Recall: 0.56\n",
      "F-score: 0.65\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       0.14      0.57      0.22       220\n",
      "    Positivo       0.95      0.57      0.72      3268\n",
      "      Neutro       0.15      0.43      0.22       317\n",
      "\n",
      "    accuracy                           0.56      3805\n",
      "   macro avg       0.41      0.52      0.39      3805\n",
      "weighted avg       0.84      0.56      0.65      3805\n",
      "\n",
      "\n",
      "Random oversampling:\n",
      "Resultados da validação cruzada:\n",
      "Scores: [0.81960734 0.80377407 0.80364656 0.81040418 0.8115517 ]\n",
      "Precisão média: 0.8097967714562492\n",
      "Acuracia: 0.68\n",
      "Precisão: 0.82\n",
      "Recall: 0.68\n",
      "F-score: 0.74\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       0.17      0.42      0.24       220\n",
      "    Positivo       0.93      0.73      0.82      3268\n",
      "      Neutro       0.17      0.38      0.24       317\n",
      "\n",
      "    accuracy                           0.68      3805\n",
      "   macro avg       0.42      0.51      0.43      3805\n",
      "weighted avg       0.82      0.68      0.74      3805\n",
      "\n",
      "\n",
      "SMOTE:\n",
      "Resultados da validação cruzada:\n",
      "Scores: [0.59854666 0.70648986 0.64783884 0.65077139 0.70521484]\n",
      "Precisão média: 0.6617723180757398\n",
      "Acuracia: 0.80\n",
      "Precisão: 0.78\n",
      "Recall: 0.80\n",
      "F-score: 0.79\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       0.19      0.16      0.17       220\n",
      "    Positivo       0.87      0.91      0.89      3268\n",
      "      Neutro       0.18      0.12      0.15       317\n",
      "\n",
      "    accuracy                           0.80      3805\n",
      "   macro avg       0.42      0.40      0.40      3805\n",
      "weighted avg       0.78      0.80      0.79      3805\n",
      "\n",
      "\n",
      "ADASYN:\n",
      "Resultados da validação cruzada:\n",
      "Scores: [0.56937492 0.68938519 0.70093698 0.69836991 0.68476447]\n",
      "Precisão média: 0.6685662944423052\n",
      "Acuracia: 0.81\n",
      "Precisão: 0.78\n",
      "Recall: 0.81\n",
      "F-score: 0.79\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       0.19      0.15      0.17       220\n",
      "    Positivo       0.87      0.92      0.90      3268\n",
      "      Neutro       0.19      0.12      0.15       317\n",
      "\n",
      "    accuracy                           0.81      3805\n",
      "   macro avg       0.42      0.40      0.40      3805\n",
      "weighted avg       0.78      0.81      0.79      3805\n",
      "\n",
      "\n",
      "Black Transplantation (SMOTEENN):\n",
      "Resultados da validação cruzada:\n",
      "Scores: [0.71013289 0.7051495  0.70127353 0.70957918 0.69648297]\n",
      "Precisão média: 0.7045236147823204\n",
      "Acuracia: 0.15\n",
      "Precisão: 0.82\n",
      "Recall: 0.15\n",
      "F-score: 0.14\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negativo       0.09      0.47      0.16       220\n",
      "    Positivo       0.94      0.08      0.14      3268\n",
      "      Neutro       0.09      0.69      0.16       317\n",
      "\n",
      "    accuracy                           0.15      3805\n",
      "   macro avg       0.37      0.41      0.15      3805\n",
      "weighted avg       0.82      0.15      0.14      3805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "\n",
    "# Carregar os dados\n",
    "data = pd.read_csv('new_comentarios_ingles.csv')\n",
    "df_1 = data['Comentario']\n",
    "rotulos = data['sentimento']\n",
    "\n",
    "df['Comentario'] = df['Comentario'].apply(lambda x: x.lower())\n",
    "\n",
    "# Definir polarizações de sentimento\n",
    "def define_sentiment(rating):\n",
    "    if rating == 'positivo':\n",
    "        return 1\n",
    "    elif rating == 'neutro':\n",
    "        return 2\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "data['sentimento_polarizado'] = data['sentimento'].apply(define_sentiment)\n",
    "rotulos = data['sentimento_polarizado']\n",
    "\n",
    "# Tokenização e lematização\n",
    "comentarios_tokenizados = []\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [wnl.lemmatize(plv) for plv in tokens]\n",
    "\n",
    "for tok in df_1:\n",
    "    tokens = nltk.word_tokenize(str(tok))\n",
    "    comentarios_tokenizados.append(tokens)\n",
    "\n",
    "lemm_tks = [lemmatize_tokens(tokens) for tokens in comentarios_tokenizados]\n",
    "\n",
    "# Divisão em conjuntos de treinamento e teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(lemm_tks, rotulos, test_size=0.2, random_state=42, stratify=rotulos)\n",
    "\n",
    "# Vetorização dos dados de texto\n",
    "vectorizer = CountVectorizer()\n",
    "X_treino_vect = vectorizer.fit_transform([' '.join(comentario) for comentario in X_treino])\n",
    "X_teste_vect = vectorizer.transform([' '.join(comentario) for comentario in X_teste])\n",
    "\n",
    "\n",
    "# Função para treinamento e avaliação do modelo\n",
    "def train_and_evaluate_model(X_train_res, y_train_res, X_test_vect, y_test):\n",
    "    classifier = MultinomialNB()\n",
    "\n",
    "    # Validação cruzada\n",
    "    scores = cross_val_score(classifier, X_train_res, y_train_res, cv=5)  # cv=5 indica 5-fold cross-validation\n",
    "\n",
    "    # Treinamento final do modelo\n",
    "    classifier.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # Predição nos dados de teste\n",
    "    predicoes = classifier.predict(X_test_vect)\n",
    "\n",
    "    # Calcular precisão, recall e F-score\n",
    "    precision = precision_score(y_test, predicoes, average='weighted')\n",
    "    recall = recall_score(y_test, predicoes, average='weighted')\n",
    "    f_score = f1_score(y_test, predicoes, average='weighted')\n",
    "    acuracia = accuracy_score(y_test, predicoes)\n",
    "    relatorio_classificacao = classification_report(y_test, predicoes, target_names=['Negativo', 'Positivo', 'Neutro'])\n",
    "\n",
    "    # Imprimir resultados\n",
    "    print(\"Resultados da validação cruzada:\")\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Precisão média:\", scores.mean())\n",
    "    print(\"Acuracia: {:.2f}\".format(acuracia))\n",
    "    print(\"Precisão: {:.2f}\".format(precision))\n",
    "    print(\"Recall: {:.2f}\".format(recall))\n",
    "    print(\"F-score: {:.2f}\".format(f_score))\n",
    "    print(\"\\nRelatório de Classificação:\")\n",
    "    print(relatorio_classificacao)\n",
    "\n",
    "# Random Undersampling\n",
    "undersampler = RandomUnderSampler()\n",
    "X_train_res, y_train_res = undersampler.fit_resample(X_treino_vect, y_treino)\n",
    "print(\"\\nRandom Undersampling:\")\n",
    "train_and_evaluate_model(X_train_res, y_train_res, X_teste_vect, y_teste)\n",
    "\n",
    "# Random Undersampling\n",
    "oversampler = RandomOverSampler()\n",
    "X_train_res, y_train_res = oversampler.fit_resample(X_treino_vect, y_treino)\n",
    "print(\"\\nRandom oversampling:\")\n",
    "train_and_evaluate_model(X_train_res, y_train_res, X_teste_vect, y_teste)\n",
    "\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE()\n",
    "X_train_res, y_train_res = smote.fit_resample(X_treino_vect, y_treino)\n",
    "print(\"\\nSMOTE:\")\n",
    "train_and_evaluate_model(X_train_res, y_train_res, X_teste_vect, y_teste)\n",
    "\n",
    "# ADASYN\n",
    "adasyn = ADASYN()\n",
    "X_train_res, y_train_res = adasyn.fit_resample(X_treino_vect, y_treino)\n",
    "print(\"\\nADASYN:\")\n",
    "train_and_evaluate_model(X_train_res, y_train_res, X_teste_vect, y_teste)\n",
    "\n",
    "smote_enn = SMOTEENN()\n",
    "X_treino_res, y_treino_res = smote_enn.fit_resample(X_treino_vect, y_treino)\n",
    "print(\"\\nBlack Transplantation (SMOTEENN):\")\n",
    "train_and_evaluate_model(X_treino_res, y_treino_res, X_teste_vect, y_teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a608e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\lab\\appdata\\roaming\\python\\python39\\site-packages (0.12.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\lab\\appdata\\roaming\\python\\python39\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.7.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
